%% grundlagen.tex
%% $Id: grundlagen.tex 61 2012-05-03 13:58:03Z bless $
%%

\chapter{Principles of compression}
\label{ch:Principles of compression}
%% ==============================
\par{
The basic idea of compression is to remove redundancy in data. Compression can be broken down into two broad categories: Lossless and lossy compression. Lossless compression makes it possible to reproduce the original data exactly while lossy compression allows the some degradation in the encoded data to gain even higher compression at the cost of some of the original information. To understand compression, one first has to understand some basic principles of information theory like Entropy and different approaches to compress different types of data with different encoding. We will also show the key differences between probability coding and dictionary coding.}

%% ==============================
\section{Compression and Encoding fundamentals}
%% ==============================
\label{ch:Principles of compression:sec:Compression}
TBD\\
...

\subsection{Information Theory and Entropy}
\par{
As Shannon described his analysis about the English language \cite{entropy}, he used the term Entropy closely aligned with its Definition in classical physics, where it is defined as the disorder of a system. Specifically speaking, it is assumed that a system has a set of states it can be in and it exists a probability distribution over those states. Shannon then defines the Entropy as:
\[
H(S) = \sum_{s \in S} P(s) \: i(s)
\]
where $S$ describes all possible States, $P(s)$ is the likelihood of $s \in S$. So generally speaking it means that evenly distributed probabilities imply a higher Entropy and vice versa.
It also implies that, given a source of information $S$, its average information content per message from $S$ is also described by this formula.}

\par{
In addition to that, Shannon defined the term self information $i(s)$ as:
\[
i(S) = log_{2} \: \frac{1}{P(s)}
\]
indicating that the higher the probability of a state, less information can be contained. As an example, the statement \enquote{The criminal is smaller than 2 meters.} is very likely but doesn't contain much information, whereas the statement \enquote{The criminal is larger than 2 meters.} is not very likely but contains lots of information. With these definitions in mind, we can analyze some properties for the English language from an information theory perspective of view.
}

\par{
Combining those approaches, you will find that $\sum$ being a finite alphabet, and $P(s_i)$ describing the likelihood of $s_i$ and $i(s_i)$ its self information, $i(s_i)$ also describes the amount of bits needed for this symbol, therefore its $log_2$.
\[
	H(\sum) = \sum_{i = 1}^{n} P(s_i) \: \cdot \: log_{2} \: \frac{1}{P(s_i) }
\]
So the entropy 	also describes the theoretical amount of bits needed to persist a message from $\sum$, calculated in $bps$ as Bits per Symbol.

TODO: fix caption

\begin{center}
\begin{tabular}[p]{l|l|l|l|l|l|r}
%	\caption{Beschreibung der Tabelle}
	
	& $P(a)$ & $P(b)$ & $P(c)$ & $P(d)$ & $P(e)$ & $H$ \\
	\hline
	1. & 0.2 & 0.2 & 0.2 & 0.2 & 0.2 & 2.322 \\
	2. & 0.94 & 0.01 & 0.01 & 0.01 & 0.01 & 0.322
	\label{tab:heisetabelle}
\end{tabular}
\end{center}
TODO: describe some relations between probability and entropy of information source
}
\subsection{General Analysis}
\par{
To evaluate the efficiency of a specific compression technique, we have to determine how much information the raw data contains. In this case for textual compression at first, we are talking about the English language. There have been broad analysis of ASCII Entropy, consisting of 96 different printable symbols for the English language, generating approaches for calculating the entropy \cite{entropy-fernau}.
}

\par {
If we assume a 96 Symbol alphabet and a uniform probability distribution, we have a quite high entropy of $log_2 (96) = 6,6 \: bps$. In a empirically distribution generated by text analysis, the entropy comes down to $4.5 \: bps$. Using an encoding with separated encoding per Symbol like the Huffman Coding, we can archive an entropy of $4.7 \: bps$ which is only slightly worse that the theoretical entropy. By changing the assumption to blocks of symbols of length 8, we get $96^8$ different blocks. With a probability distribution close to the English language we get an entropy as low as $1.3 \: bps$ which implies a possible reduction of symbols to 3 printable characters without generating longer texts.}

\par{
Consulting newer sources we will find that, up to 500 character long symbol based text analysis, with around $20.3 \: Mio.$ different symbols, results in an entropy of around $1,58 \: bps$ \cite{entropy-new}. This gives a clear limit of how much compression we can theoretically expect from English text.  Using a combination of different approaches like Huffman Coding and Run Length Coding, and encoding different bits of a single symbol with different approaches, we do no longer encode every symbol separately, which might result in a better compression.}


\subsection{Probabilistic Coding}
\par{
The general idea behind Probability Coding is to analyze probabilities for messages and the encode them in bit strings according to their probability. This way, messages that are more likely and will repeat more often can be encoded in a smaller bit string representation. The generation of those probability distributions is considered part of the Analyzer module by the algorithm and will be discussed later on (chap: design, chap: impl). Probability coding compression can be discerned into fixed unique coding, which will represent each message with a bit string with the amount of bits being an integer, for example Huffman coding as type of prefix coding. In contrast to Huffman coding there are also arithmetic codes which can represent a message with a floating point number of bits in the corresponding bit string. By doing so they can \enquote{fuse} messages together and need less space to represent a set of messages.
}
...


\subsection{Dictionary Coding}

\par{
Dictionary Coding is best suited for Data with a small amount of repeating patterns like a text representing specific words. In this case it is very effective to save the patterns just once and refer to them if the are used later on in the text. If we know a lot about the text itself in advance, a static dictionary is sufficient but in general we want to compress an arbitrary text. This requires a more general approach as used by the well known LZ77 and LZ88 algorithms described by Jacob Ziv and Abraham Lempel \cite{lz}. They use a so called sliding window principle, a buffer moving across the data to encode in the case of LZ77 or a dynamic dictionary in the implementation of LZ78. Both of them are still used in modified versions in real world applications as described in Section \ref{ch:Principles of compression:sec:SOTA}.
}
\par{
The main difference between probabilistic coding and dictionary coding is, that the so far presented probability based methods like RLE or Huffman Coding are working on single character or byte whereas the dictionary based methods encode groups of characters of varying length. This is a clear advantage in terms of compression performance on every day data because of its repeating patterns most textual data has, as shown in Section \ref{ch:Analysis:sec:Initial Findings}.
}


\subsection{Irreversible Compression}
\par{
Irreversible Compression or also called \enquote{Lossy Compression} is a type of compression which looses information in the compression process and is therefore not completely reversible, hence the name. There are a lot of use cases for these type of compression algorithms, mostly used for images, video and audio files. These files typically contain information almost not perceptible for an average human like really small color differences between two pixels of an image or very high frequencies in an audio file. By using approximations to store the information and accept the loss of some information while retaining most of it, lossy image compression algorithms can get twice the compression performance compared to lossless algorithms. Due to the fact that most lossy compression techniques are not suited for text compression which is the main use case for this work, we will not elaborate any further on this topic.
}


%% ==============================
\section{Run Length Coding}
%% ==============================
\label{ch:Principles of compression:sec:Run Length Encoding}
\par{
	Run length coding might count as the simplest coding scheme that makes use of context and repeating occurrences as described in the Section \ref{ch:Introduction:sec:Motivation}. While the example made earlier was in textual representation, it is best suited and mostly used for pallet based bitmap images \cite{palette-image} such as fax transmissions or computer icons.
}

\subsection{The history}
\label{ch:Principles of compression:sec:Run Length Encoding:subSec:History}
\par{
 The ITU-T T4 (Group 3) standard for Facsimile (fax) machines \cite{ITU} is still in force for all devices used over regular phone lines. Each transmission sends a black and white image, where each pixel is called a \textit{pel} and with a horizontal resolution of $8.05 \: \frac{pels}{mm}$ and the vertical resolution depending on the mode. To encode each sequence of black and white pixels, the T4 standard uses RLE to encode each sequence of black and white pixels and since there are only two values, only the run length itself has to be encoded. It is assumed that the first run is always a white run, so there is a dummy white pel at the beginning of each sequence. For example, the sequence $bbbbwwbbbbb$ can be encoded as 1,4,2,5 with the leading white dummy pixel.
}
	
\par{
To further reduce the RLE encoded sequence, a probability coding procedure like Huffman coding can be used do code the sequence, since shorter runs like lengths are generally more common than very long runs, which is also done by the T4 standard. Based on a broad analysis of these run lengths, the T4 defined a set of static Huffman codes, using a different codes for the black and white pixels.
}
\begin{center}
	\begin{tabular}[p]{r|l|l}
		%	\caption{Beschreibung der Tabelle}
		
		run length &  white codeword & black codeword\\
		\hline
		0 &  00110101 & 0000110111\\
		1 & 000111 & 010\\
		2 & 0111 & 11\\
		3 & 1000 & 10\\
		4 & 1011 & 011\\
		... &  & \\
		20 & 0001000 & 00001101000\\
		... & & \\
		64+ & 11011 & 0000001111\\
		128+ & 10010 & 000011001000
		\label{tab:t4 run length Huffman codes}
	\end{tabular}
\end{center}

\par{
A small subset of the static T4 Huffman codes are given in Table \ref{tab:t4 run length Huffman
	codes} which also shows the most likely runs to occur. Runs of more than 64 have to be encoded in multiple Huffman codes, for example a run of 150 has to be encoded with the Huffman code of 128 followed by the code for 22.
}
\par{
By combining RLE with a probabilistic approach, the ratio of compression rises because we no longer have to encode a run of length 1 or 2 with a fixed size of bits each instead we can write a recognizable Huffman code of varying length, which will be explored in grater detail later on as well as the decoding of the shown Huffman codes.}

\subsection{Limitations}
\par{
As mentioned in the Section \ref{ch:Introduction:sec:Problem statement}, run length encoding is rarely used for regular text or continuous tone images because its potentially increase in size, due to non repetitive characters or bytes. To reduce this problem there are several approaches known, partly implemented and used some of which will be addressed like a Burrows-Wheeler-Transformation.
}

\subsection{Run Length Encoding today}
\par{
While it is still in use by fax transmission or other highly specific tasks, it is mostly used in combination with other approaches. 
TODO: give more references
}
%% ==============================
\section{Prefix Coding}
%% ==============================
\label{ch:Principles of compression:sec:Huffman Coding}
\par{
Prefix codes make use of an untypical idea in computer science. Usually we deal with fixed length codes like 7 bit ASCII or 32 bit Integer representations which map each possible value into that fixed amount of bits. For compression it would be a benefit if we could write codes of variable length. The problem with these varying length codes is that as soon as they are part of a sequence, it becomes very hard to tell where one code starts and finishes, resulting in ambiguous encoding. For example the set of codes $ \{(a,1),(b,01),(c,101),(d,011)\}$ and the encoded message is $1011$, there is no way to tell which message was encoded.
}
\par{
One way to address this problem is by adding extra stop symbols or encoding a length before each code but those just add additional data. Another approach is to use so called prefix codes and make no code which is prefix of another code, which makes them uniquely decodable as show in the next section.
}



\subsection{Huffman Coding}
\par{
Huffman coding 
}

TODO: fix dang tree to be able to have only one child node
\begin{center}
	\begin{tikzpicture}[->,>=stealth',level/.style={sibling distance = 5cm/#1,
		level distance = 1.5cm}] 
	\node [arn_n] {}
	child{ node [arn_n] {1} 
		child{ node [arn_n] {1} 
			child{ node [arn_n] {b}}
		}
		child{ node [arn_n] {0}
			child{ node [arn_n] {a}}
		}                            
	}
	child{ node [arn_n] {0}	
			child{ node [arn_n] {c}}
	}
	; 
	\end{tikzpicture}
\end{center}

\par{
TODO: explain stuff
}
...

\section{Other methods}
\label{ch:Principles of compression:sec:Other}
\par{
There are other well known methods like \\
- move to front\\
- arithmetic coding\\
- real world applications are mostly combinations of those\\
}
...
\subsection{Burrows-Wheeler-Transformation}
\par{
\label{ch:PrPrinciples of compression:sec:Other:subSec:bwt}
The Burrows-Wheeler-Transformation is not a compression technique but rather a method to prepare text to increase its compression potential, described by M. Burrows and D. J. Wheeler \cite{Burrows94}. It is a Transformation of a string $S$ of $n$ characters by forming the $n$
rotations of S, sorting them lexicographically, and extracting the last
character of each of the rotations. The result of this transformation is a string L, consisting of these last characters of each sorted rotation. The algorithm also has to add special termination symbols or compute the index $I$ of the original string S in the sorted list of
rotations. Surprisingly, there is an efficient algorithm to invert the transformation back to the original string S given only L and the Index $I$ \cite{Burrows-linear-time}.
}

\par {
As an example of the creation of L, given the input string S = ‘abraca’, $n = 6$, and the alphabet
$X = \{a,b,c,r\}$. Create a $N \times N$ matrix $M$ whose elements are characters, and
whose rows are rotations of S. Sort all rotations in lexicographical order.
}

\par{
In this example, the index is $I = 1$ and the matrix $M$ is

\begin{center}
	\begin{tabular}[p]{r|l|l|l|l|l|l}
		%	\caption{Beschreibung der Tabelle}
		
		row 0 & a & a & b & r & a & c\\
		\hline
		row 1 & a & b & r & a & c & a\\
		\hline
		row 2 & a & c & a & a & b & r\\
		\hline
		row 3 & b & r & a & c & a & a\\
		\hline
		row 4 & c & a & a & b & r & a\\
		\hline
		row 5 & r & a & c & a & a & b

		\label{tab:t10 bwt-example}
	\end{tabular}
\end{center}


The resulting string L corresponds to the last column of $M$, with characters $M[0,n -1],\: \dots \: ,M[n - 1, n - 1]$. The output of the transformation is the pair $(L, I)$, in the example, $L = ‘caraab’$ and $I = 1$. Obviously the string L contains consecutive identical characters, which result in better compressibility as shown in \ref{ch:Analysis:sec:Improvements by Preprocessing:subSec:bwt}.
}

\par{
TODO: explain reversing a bwt
}

%% ==============================
\section{State of the Art}
%% ==============================
\label{ch:Principles of compression:sec:SOTA}
State of the art compression\\
- techniques\\
- use cases\\
- limits\\

\par{
	
	\begin{center}
		\begin{tabular}[p]{r|l|l|l}
			%	\caption{Beschreibung der Tabelle}
			
			method &  size in bytes & compression & ratio in $\frac{bits}{symbol}$\\
			\hline
			uncompressed & 3,141,622 & 100\% & 8.00\\
			compress & 1,272,772 & 40.0\% & 3.24\\
			ZIP v2.32 & 1,020,781 & 32.4\% & 2.59\\
			gzip v1.3.5 & 1,017,624 & 32.3\% & 2.58\\
			bzip2 v9.12b & 828,347 & 26.3\% & 2.10 \\
			ppmd & 740,737 & 23.5\% & 1.88\\
			ZPAQ v7.15& 659,709 & 20.9\% & 1.67 
			
			\label{tab:t20 stat of the art}
		\end{tabular}
	\end{center}

}

%% ==============================
\section{Limits of compression}
%% ==============================
\label{ch:Principles of compression:sec:Limits of Conpression}

- existence of not compressable strings\\
-\url{https://www.quora.com/Is-there-a-theoretical-limit-to-data-compression-If-so-how-was-it-found} \\
- unable to compress random data\\
- Kolmogorow-Komplexität	

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
