%% grundlagen.tex
%% $Id: grundlagen.tex 61 2012-05-03 13:58:03Z bless $
%%

\chapter{Principles of compression}
\label{ch:Principles of compression}
%% ==============================

To understand compression, one first has to understand some basic principles of information theory like Entropy and different approaches to compress different types of data with different encoding and entropy. I will also show the key differences between probability coding and dictionary coding and a few comments on lossy compression.

%% ==============================
\section{Compression and Encoding fundamentals}
%% ==============================
\label{ch:Principles of compression:sec:Compression}
TBD\\
...

\subsection{Information Theory and Entropy}
\par{
As Shannon described his analysis about the English language \cite{entropy}, he used the term Entropy closely aligned with its Definition in classical physics, where it is defined as the disorder of a system. Specifically speaking, it is assumed that a system has a set of states it can be in and it exists a probability distribution over those states. Shannon then defines the Entropy as:
\[
H(S) = \sum_{s \in S} P(s) \: i(s)
\]
where $S$ describes all possible States, $P(s)$ is the likelihood of $s \in S$. So generally speaking it means that evenly distributed probabilities imply a higher Entropy and vice versa.
It also implies that given a source of information $S$, its average information content per message from $S$ is also described by this formula.}

\par{
In addition to that, Shannon defined the term self information $i(s)$ as:
\[
i(S) = log_{2} \: \frac{1}{P(s)}
\]
indicating that the higher the probability of a state, less information can be contained. As an example, the statement \enquote{The criminal is smaller than 2 meters.} is very likely but doesn't contain much information, whereas the statement \enquote{The criminal is larger than 2 meters.} is not very likely but contains lots of information. With these definitions in mind, we can analyze some properties for the English language from an information theory perspective of view.
}

\par{
Combining those approaches, you will find that $\sum$ being a finite alphabet, and $P(s_i)$ describing the likelihood of $s_i$ and $i(s_i)$ its self information, $i(s_i)$ also describes the amount of bits needed for this symbol, therefore its $log_2$.
\[
	H(\sum) = \sum_{i = 1}^{n} P(s_i) \: \cdot \: log_{2} \: \frac{1}{P(s_i) }
\]
So the entropy 	also describes the theoretical amount of bits needed to persist a message from $\sum$, calculated in $bps$ as Bits per Symbol.

TODO: fix caption

\begin{tabular}[p]{l|l|l|l|l|l|r}
%	\caption{Beschreibung der Tabelle}
	
	& $P(a)$ & $P(b)$ & $P(c)$ & $P(d)$ & $P(e)$ & $H$ \\
	\hline
	1. & 0.2 & 0.2 & 0.2 & 0.2 & 0.2 & 2.322\\
	2. & 0.94 & 0.01 & 0.01 & 0.01 & 0.01 & 0.322\\
	\label{tab:heisetabelle}
\end{tabular}

TODO: describe some relations between probability and entropy of information source
}
\subsection{General Analysis}
\par{
To evaluate the efficiency of a specific compression technique, we have to determine how much information the raw data contains. In this case for textual compression at first, we are talking about the English language. There have been broad analysis of ASCII Entropy, consisting of 96 different printable symbols for the English language, generating approaches for calculating the entropy \cite{entropy-fernau}.
}

\par {
If we assume a 96 Symbol alphabet and a uniform probability distribution, we have a quite high entropy of $log_2 (96) = 6,6 \: bps$. In a empirically distribution generated by text analysis, the entropy comes down to $4.5 \: bps$. Using an encoding with separated encoding per Symbol ly the Huffman Coding, we can archive an entropy of $4.7 \: bps$ which is only slightly worse that the theoretical entropy. By changing the assumption to blocks of symbols of length 8, we get $96^8$ different blocks. With a probability distribution close to the English language we get an entropy as low as $1.3 \: bps$ which implies a possible reduction of symbols to 3 printable characters without generating longer texts.}

\par{
Consulting newer sources we will find that, up to 500 character long symbol based text analysis, with around $20.3 \: Mio.$ different symbols, results in an entropy of around $1,58 \: bps$ \cite{entropy-new}. Using a combination of different approaches like Huffman Coding and Run Length Coding, and encoding different bits of a single symbol with different approaches, we do no longer encode every symbol separately, which might result in a better compression.}


\subsection{Probability Coding}
\par{
The general idea behind Probability Coding is to analyze probabilities for messages and the encode them in bit strings according to their probability. This way, messages that are more likely and will repeat more often can be encoded in a smaller bit string representation. The generation of those probability distributions is considered part of the Analyzer module by the algorithm and will be discussed later on (chap: design, chap: impl). Probability coding compression can be discerned into fixed unique coding, which will represent each message with a bit string with the amount of bits being an integer, for example Huffman coding as type of prefix coding. In contrast to Huffman coding there are also arithmetic codes which can represent a message with a floating point number of bits in the corresponding bit string. By doing so they can \enquote{fuse} messages together and need less space to represent a set of messages.
}
...


\subsection{Dictionary Coding}
...
\subsection{Irreversible Compression}
\par{
Irreversible Compression or also called \enquote{Lossy Compression} is a type of compression which looses information in the compression process and is therefore not completely reversible, hence the name. There are a lot of use cases for these type of compression algorithms, mostly used for images, video and audio files. These files typically contain information almost not perceptible for an average human like really small color differences between two pixels of an image or very high frequencies in an audio file. By using approximations to store the information and accept the loss of some information while retaining most of it, lossy image compression algorithms can get twice the compression performance compared to lossless algorithms. Due to the fact that most lossy compression techniques are not suited for text compression which is the main use case for this work, we will not elaborate any further on this topic.
}


%% ==============================
\section{Run Length Encoding}
%% ==============================
\label{ch:Principles of compression:sec:Run Length Encoding}

...

%% ==============================
\section{Huffman Coding}
%% ==============================
\label{ch:Principles of compression:sec:Huffman Coding}

...

%% ==============================
\section{State of the Art}
%% ==============================
\label{ch:Principles of compression:sec:SOTA}
Die Literaturrecherche soll so vollständig wie möglich sein und bereits existierende relevante Ansätze (Verwandte Arbeiten / State of the Art / Stand der Technik) beschreiben bzw. kurz vorstellen.
Es soll aufgezeigt werden, wo diese Ansätze Defizite aufweisen oder nicht anwendbar sind, z.B. weil sie von anderen Umgebungen oder Voraussetzungen ausgehen.

Je nach Art der Abschlussarbeit kann es auch sinnvoll sein, diesen Abschnitt in die Einleitung zu integrieren oder als eigenes Kapitel aufzuführen.



State of the art compression\\
- techniques\\
- use cases\\


%% ==============================
\section{Limits of compression}
%% ==============================
\label{ch:Principles of compression:sec:Limits of Conpression}

- existence of not compressable strings\\
-\url{https://www.quora.com/Is-there-a-theoretical-limit-to-data-compression-If-so-how-was-it-found} \\
- unable to compress random data\\
- Kolmogorow-Komplexität	

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
