%% analyse.tex
%% $Id: analyse.tex 61 2012-05-03 13:58:03Z bless $

\chapter{Analysis}
\label{ch:Analysis}
%% ==============================

\par{
The following chapter contains a detailed analysis of the problem and some fundamental requirements for the algorithm. 
}

%% ==============================
\section{Initial Findings}
%% ==============================
\label{ch:Analysis:sec:Initial Findings}
\par{
 Originally developed and used on black and white pallet images containing only two values, on a binary level so to speak, we want to use it for rather arbitrary data, mostly text. The initial algorithm is not suited for that because continuous text as binary representation does not contain runs of any kind, which could be compressed. The ASCII representation of the letter 'e' which is the most common in general English, has the value 130 or '01100101' as 7-bit ASCII or '001100101' as byte value of the UTF-8 representation. And this is the case for most printable characters as they al have a value between 32 and 127 (or 255 for the extended ASCII).
}

\par{
RLE could also work on a byte level, because there should be repetitions of any kind. Byte level RLE encodes runs of identical byte values, ignoring individual bits and word boundaries. The most common byte level RLE scheme encodes runs of bytes into 2-byte packets. The first byte contains the run count of 1 to 256, and the second byte contains the value of the byte run. If a run exceeds a count of 256, it has to be encoded twice, one with count 256 and one with any further runs.
}

\par{
 However after some analysis of the corpus data, it was shown that most runs had a value of one and almost no runs larger that 4 occurred, which lead to the conclusion, two bit for the run count should be plenty, which is also shown in Table \ref{tab:t5 run length eval}.
To have a comparison to some extent, the initial performance analysis was performed on the Calgary Corpus but the results where very underwhelming as expected, as shown in the table below. Even with a run size of just two bits, there is still a increase in size of about 9\% and uses 8.74 $\frac{bits}{symbol}$. This is still useful as a kind of a base line.
}

\begin{center}
	\begin{tabular}[p]{r|r|r}
				\label{tab:t5 run length eval}
		
		bits per rle number & compression ratio in \% & bits per symbol in $\frac{bits}{symbol}$\\
		\hline
		8 & 165 & 13.2 \\
		7 & 154 & 12.38\\
		6 & 144 & 11.57 \\
		5 & 134 & 10.77\\
		4 & 125 & 10.00\\
		3 & 116 & 9.29\\
		2 & 109 & 8.74 \\
	\end{tabular}
\end{center}

\ldots

%% ==============================
\section{Improvements by Preprocessing}
%% ==============================
\label{ch:Analysis:sec:Improvements by Preprocessing}
\subsection{Burrows-Wheeler-Transformation}
- conflicting results \\
- most corpus data not useable because of use characters \\
- even further increases size
\ldots

\begin{center}
	\begin{tabular}[p]{r|r|r}
		\label{tab:t5 run length eval}
		
		bits per rle number & compression ratio in \% & bits per symbol in $\frac{bits}{symbol}$\\
		\hline
		3 & 231 & 18\\
		2 & 206 & 16.5\\
	\end{tabular}
\end{center}


\subsection{Vertical byte reading}
\par{
Instead of performing compute intense operation on the data, we could also interpret the data in a different way and apply the original run length encoding on binary data. By reading the data in chunks of a fixed size, it is possible to read all most significant bits of all bytes, then the second most significant bits of all bytes and so on. This interpretation results in longer runs as shown in example X.
}

\par{

}


%% ==============================
\section{Summary}
%% ==============================
\label{ch:Analyse:sec:Summary}

Am Ende sollten ggf. die wichtigsten Ergebnisse nochmal in \emph{einem}
kurzen Absatz zusammengefasst werden.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
