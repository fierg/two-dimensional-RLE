%% analyse.tex
%% $Id: analyse.tex 61 2012-05-03 13:58:03Z bless $

\chapter{Analysis}
\label{ch:Analysis}
%% ==============================

\par{
The following chapter contains a detailed analysis of the problem and some fundamental requirements for the algorithm. To have a comparison to some extent, the initial performance analysis was performed on the Calgary Corpus but the results of the unmodified run length coding algorithm where very underwhelming as expected.
}

%% ==============================
\section{Initial Findings}
%% ==============================
\label{ch:Analysis:sec:Initial Findings}
\par{
 Originally developed and used on black and white pallet images containing only two values, on a binary level so to speak, we want to use it for rather arbitrary data, mostly text. The initial algorithm is not suited for that because continuous text as binary representation does not contain runs of any kind, which could be compressed. The ASCII representation of the letter 'e' which is the most common in general English, has the value 130 or '01100101' as 7-bit ASCII or '001100101' as byte value of the UTF-8 representation. As you can see, there are no runs of a considerable size and this is the case for most printable characters as they all have a value between 32 and 127 (or 255 for the extended ASCII). Applied to the Calgary Corpus in this simple implementation, there should be an increase in size expected or \textit{negative compression} as one might say.
}

\begin{table}[h]
	\centering
	\begin{tabular}{l|r|r}
		\label{tab:t4 simple run length eval}
		
		bits per rle number &  expansion ratio & bits per symbol in $\frac{bits}{symbol}$\\
		\hline
		8 & 3.298046423741734& 26.38437138993387 \\
		7 & 2.8897459975751163& 23.11796798060093\\
		6 & 2.4839251325134675& 19.87140106010774 \\
		5 & 2.0825541259578895 & 16.66043300766311\\
		4 & 1.6895640359371056 & 13.516512287496845\\
		3 & 1.3130541262757818 & 10.504433010206254\\
		2 & 1.04555716691706 & 8.36445733533648 \\
	\end{tabular}
	\caption{Binary RLE on the Calgary Corpus}
\end{table}

\par{
The results in Table \ref{tab:t4 simple run length eval} depict the anticipated, an increase in size regardless of the amount of bits used to encode a run. By using 8 bits to encode a single run, in the worst case scenario a byte which is only alternating values like 01010101, would expand to 8 bytes, all encoding a run of length 1. For this reason, many implementations combine RLE with other encoding schemes like Huffman encding to be able to encode runs with variable length.
}

\par{
RLE is also applicable on a byte level, because there should be repetitions of any kind like consecutive letters or line endings (EOL). Byte level RLE encodes runs of identical byte values, ignoring individual bits and word boundaries. The most common byte level RLE scheme encodes runs of bytes into 2-byte packets. The first byte contains the run count of 1 to 256, and the second byte contains the value of the byte run. If a run exceeds a count of 256, it has to be encoded twice, one with count 256 and one with any further runs.
}

\begin{table}[h]
	\centering
	\begin{tabular}{l|r|r}
		\label{tab:t5 run length eval}
		
		bits per rle number &  ratio in \% & bits per symbol in $\frac{bits}{symbol}$\\
		\hline
		8 & 165 & 13.2 \\
		7 & 154 & 12.38\\
		6 & 144 & 11.57 \\
		5 & 134 & 10.77\\
		4 & 125 & 10.00\\
		3 & 116 & 9.29\\
		2 & 109 & 8.74 \\
	\end{tabular}
	\caption{Byte-wise RLE on the Calgary Corpus}
\end{table}


\par{
 However after some analysis of the corpus data, it was shown that most runs had a value of one and almost no runs larger that 4 occurred, which lead to the conclusion, two bit for the run count should be plenty, which is also shown in Table \ref{tab:t5 run length eval}.
 Even with a run size of just two bits, there is still a increase in size of about 9\% and uses 8.74 $\frac{bits}{symbol}$. This is still useful as a kind of a base line.
Interestingly the binary implementation performs better on 2 bits per RLE number (4 \% increase in size) than the byte implementation (9 \% increase in size) but also worse with a higher amount of bits per run, where it expands the data to more than triple in size. It is unclear which kind of implementation will profit most of preprocessing, so both will be further analyzed, but the benefit of byte wise RLE is the better worst case performance of 1.5 up to 2 times the original size compared to binary RLE with up to 4.5 times the original size.
}

\par{
If we take a more detailed look, we can see that while most files expand with larger RLE numbers, some files have their minimum size when encoded with higher RLE numbers of up to 7 bit. With the simple binary based RLE, almost all files of the Calgary Corpus expand linear related to the amount of bits used for the encoding. However the file \textit{pic} decreases in size until 7 bits per RLE number used to a sizes of just $19.5 \%$ of its original size with only  $1.56 \: \frac{bits}{symbol}$ while the other files just doubled or even tripled in size. Using the byte wise operating RLE we see a similar result with the file \textit{pic}, but not as decent with $27.2\%$ of its original size using $2.17 \: \frac{bits}{symbol}$ encoding with 6 bits per run.

\begin{table}[h]
	\centering
	\begin{tabular}{r|r|r|r|r|r}	
		file & size original & $\frac{bits}{RLE number}$ & size encoded & ratio in \% & $\frac{bits}{symbol}$\\
		\hline
		pic & 513216 & 2 & 350292 & 68.25 & 5.46 \\
		 & & 3 & 235067 & 45.80 & 3.66\\
		 & & 4 & 165745 & 32.29 & 2.58\\
		 & & 5 & 126349 & 24.61 & 1.96\\
		 & & 6 & 106773 & 20.80 & 1.66\\
		 & & 7 & 100098 & 19.50 & 1.56\\
		 & & 8 & 101014 & 19.68 & 1.57\\		 
	\end{tabular}
	\caption{The file \textit{pic} with increasing bits per RLE encoded number}
	\label{tab:t40 The file pic with increasing bits per RLE encoded number}
\end{table}	
}

%% ==============================
\section{Possible Improvements by Preprocessing}
%% ==============================
\label{ch:Analysis:sec:Improvements by Preprocessing}

The broad idea of preprocessing is to manipulate the input data in a way that results in data which can be compressed more efficiently than the original data. This can be done in various ways, some of them will be explored in greater detail to find out if it is implementable or not. One way of doing so is a Burrows-Wheeler-Transformation.

\subsection{Burrows-Wheeler-Transformation}
\label{ch:Analysis:sec:Improvements by Preprocessing:subSec:bwt}
\par{
To understand how a Burrows-Wheeler-Transformation improves the effectiveness of compression, consider the effect in a common word in English text. Examine the letter ‘t’ in the word ‘the’, in an input string holding multiple instances of this word.
Sorting all rotations of a string results in all rotations starting with ‘he ’ will be sorted together and most of them are going to to end in the letter ‘t’. This implies that the transformed string L has a large number of the letter t, combined with some other characters, such as space, ‘s’, ‘T’, and ‘S’. This is true for all characters, so any substring of L is likely to contain a large number of a some distinct characters. \enquote{The overall effect is that the probability that given character $c$ will occur at a given
point in L is very high if $c$ occurs near that point in L, and is low otherwise.} \cite{Burrows94}
}

\par{
It is obvious that this should always improve the performance of byte level RLE because the transformation is taking place at character level but it should not effect the binary implementations. 
}

\subsection{Vertical byte reading}
\par{
Instead of performing compute intense operation on the data, we could also interpret the data in a different way and apply the original run length encoding on binary data. This idea is also know for binary RLE on images, where the encoding in the image follows a specific path.\\
 TODO showcase}

\par{ By reading the data in chunks of a fixed size, it is possible to read all most significant bits of all bytes, then the second most significant bits of all bytes and so on. This interpretation results in longer runs as shown in the example below.
}

\par{
The binary UTF-8 interpretation of the example string from earlier S = ‘abraca’ results in 8 runs of length 1, 9 runs of length 2 as well as 3 runs of length 3 and 4.  
\scalardump\dataA

Reading the data in a different way, all most significant bits, then al second most significant bits and so forth, it results in much longer runs as shown below.
\arraydump\dataA

Now we have 5 runs of length 6, 2 runs of length 3, 3 runs of length 2 and just 6 runs of length 1 as opposed by the simple interpretation. This is because the binary similarity between the used characters, as the character for a and b only differ in one bit. It is clear that simply a different way of reading the input does not compress the actual data, instead it enables a better application of existing compression. 
}

\subsection{Byte remapping}
\par{
The effect of very long runs in the last example was mainly because the binary representations of the used characters are very similar, so the range of byte values used was very small (between a = 97 and r = 114). Introducing other used symbols like uppercase letters, space or new lines, the used range expands.
}

\par{
The binary representation of a String like S' = "Lorem ipsum dolor sit amet, consectetur adipiscing elit.", results in a worse result as shown below. 
The usage of other characters expanded the used byte range to between 32 and 117 which results in shorter average runs. Interestingly the most significant bit is always 0, a fragment from the backwards compatibility with standard ASCII encoding.

\arraydump\dataB
}

\par{
One idea to solve the shorter runs might be a dynamic byte remapping, as the input data is read in parts, where the most frequently used bytes are mapped to the lowest value. This way the values are not alternating in the whole range of 0 to 255 but rather in a smaller subset and the most frequent ones will be the smallest values, so in theory our average runs should increase because we should encounter more consecutive zeros. Some sections have more specific characters or bytes than others but this idea can also be applied to the whole file however it is unclear at this point what method outperforms which. A single map for each block of data should result in lower average values used but also creates a kind of overhead because the mapping has to be stored in the encoded file as well. Applying a simple mapping to lover values results in the following horizontally interpreted rows.
}

\par{
\arraydump\dataC

Using this method, the 4 most significant bits all result in zero columns, even row 5 has long runs while it is worth noting that the mapping itself has to be persisted in the encoded file as well. It is also still unclear if this idea scales well or is applicable to other files.
}

\subsection{Combined approaches}
\par{
The idea of combining different compression methods into a superior method is not new and was also performed on RLE as mentioned in Section \ref{ch:Principles of compression:sec:Run Length Encoding:subSec:History}. While the idea of encoding the RLE numbers with Huffman codes is already known and analyzed, it is mostly in a static sense and optimized for special purpose applications. However the vertical byte reading enables new approaches, even more in combination with the idea of byte remapping and might become applicable to more than just binary Fax transmissions or DNA sequencing with longer runs of any kind in average.
}
\par{
It might be interesting to see how well the appliance performs using the vertical binary encoding, in combination with the byte mapping. We expect the more significant the bit, the longer the runs because alternating values should be mostly on the lower significance bits. Using larger run length numbers for these rows while using smaller RLE numbers or even another encoding scheme like simple Huffman encoding we should improve our initial results.
}
\par{
Another interesting approach is the improvement archived by the combination of different methods like performing this modified RLE run on arbitrary data and then encode the results with Huffman codes, using shorter codes for more frequent runs. This idea is not new and also used in the current Fax Transmission Protocol but only for the simple binary RLE in combination with modified Huffman Codes. Other papers also mentioned these combined approaches and seemed to archived good compression ratios, not much worse than the theoretical limit of around 1.5 $\frac{bits}{symbol}$ shown in Section \ref{ch:Principles of compression:sec:Compression}. This was also done by M. Burrows and D.J. Wheeler in 1994 with their Transformation, in combination with a Move-to-Front Coder and a Huffman Coder \cite{Burrows94}. Encoding the Calgary Corpus resulted in a decrease in size to just 27\% of its original with a mean $\frac{bits}{symbol}$ of just $2.43$. This approach would no longer be considered preprocessing, but if more compression could be archived by adding a post-processing step it is still worth trying out.}

%% ==============================
\section{Summary}
%% ==============================
\label{ch:Analyse:sec:Summary}

TODO\\

Am Ende sollten ggf. die wichtigsten Ergebnisse nochmal in \emph{einem}
kurzen Absatz zusammengefasst werden.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
